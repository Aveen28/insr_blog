---
layout: post
title: "Solving Time-Dependent PDEs with Implicit Neural Spatial Representations"
categories: [deep-learning, scientific-ml, PDEs]
excerpt_separator: "<!--more-->"
---

## ðŸ§© Introduction

Solving timeâ€‘dependent partial differential equations (PDEs) is fundamental to understanding and predicting a wide range of realâ€‘world processesâ€”from the sweeping currents in the atmosphere and oceans to the flexing and cracking of materials under stress. At their core, these simulations require two key steps:

1. **Time stepping**, where the systemâ€™s state is advanced in small increments to capture its temporal evolution.  
2. **Spatial discretization**, where the continuous physical domain is broken into a finite set of points or elements (grids, meshes, or particles) so that the underlying equations can be solved numerically.

Despite decades of refinement, classical spatial discretizations still face three major challenges:

- **Numerical artifacts** such as artificial diffusion or spurious oscillations, which can distort important features and erode accuracy.  
- **Rapidly growing memory requirements**: as you refine the mesh to resolve finer details, the number of elements skyrockets and so does the cost of storing and manipulating them.  
- **Complex adaptivity**: dynamically refining or coarsening the mesh on the fly is often a monumental coding effort and a source of instabilities.

What if we could remove the mesh entirely and let a single, flexible model represent the spatial field? Thatâ€™s the promise of **Implicit Neural Spatial Representations (INSRs)**. Rather than assigning a variable to each node in a mesh, we represent the entire fieldâ€”whether itâ€™s fluid velocity, pressure, or material displacementâ€”as a continuous function encoded in the weights of a neural network. As the simulation marches forward in time, we simply update the networkâ€™s parameters according to the governing physics, using wellâ€‘established time integrators like explicit or implicit schemes.

This meshâ€‘free approach brings three standout benefits:

- **Fixed memory footprint**: the networkâ€™s size stays constant, regardless of how â€œsmoothâ€ or â€œcomplexâ€ the solution becomes.  
- **Adaptive resolution**: the neural network can automatically allocate capacity to where itâ€™s needed most, without the overhead of remeshing.  
- **Selfâ€‘contained solver**: no external training data is requiredâ€”INSR learns the solution â€œon the flyâ€ by minimizing the physics residual itself.

In this post, weâ€™ll dive into how INSRs work, explore their integration with classic timeâ€‘stepping methods, and showcase benchmark results on advection, turbulent vortex flows, and nonlinear elastic deformations. While INSRs may demand more computation per time step, they deliver higher accuracy, lower memory usage, and a simplicity of implementation that opens new doors for scientific simulation. Letâ€™s explore this exciting frontier in meshâ€‘free numerical methods.  

---

## ðŸŒŒ What Is an Implicit Neural Spatial Representation (INSR)?

![Implicit Neural Spatial Representation]({{ site.baseurl }}/images/img_1_insr.png)  
*Figure 1: An INSR encodes an entire spatial field in a neural network.*

An **Implicit Neural Spatial Representation (INSR)** is a meshâ€‘free way to represent any physical fieldâ€”velocity, pressure, deformation, etc.â€”as a single continuous function approximated by a neural network.  Instead of storing values at discrete grid points or mesh vertices, we ask:

> **â€œGiven any point in space, what is the field value there?â€**

---

### How It Works

1. **Coordinate Query**  
   - You feed the network the coordinates of one point, e.g. `(x, y)` in 2D or `(x, y, z)` in 3D.

2. **Network Inference**  
   - A multilayer perceptron (MLP) with sinusoidal or ReLU activations processes those coordinates.  
   - **All** of its weights jointly determine the outputâ€”there is no local â€œcellâ€ or â€œelement.â€

3. **Field Value Output**  
   - The network returns the physical quantity at that location (a scalar or vector).

---

### Key Properties

- **Continuous & Differentiable**  
  The network defines a \(C^\infty\) function. Computing spatial gradients, divergences, or Laplacians is just autoâ€‘diff.

- **Global Support**  
  Every weight influences the field everywhere. This global coupling lets the model capture longâ€‘range correlations naturally.

- **Fixed Memory Footprint**  
  No matter how finely you sample the domain, you only ever store the networkâ€™s weights.

- **Adaptive Detail**  
  During training, the network learns to allocate its capacity to complex regions (shocks, vortices, contact fronts) without needing to refine a mesh.

---

## âš™ï¸ Method Overview

- Neural network encodes the spatial field at each timestep
- Time integration (e.g., Euler, midpoint, variational integrator) evolves the weights
- Gradient-based optimization minimizes residuals in the PDE
- No training data needed â€” this is a solver, not a surrogate

---

## ðŸ“Š Benchmarks

### ðŸŒ€ 1. Advection Equation
- Preserves wave shape with high accuracy
- No numerical diffusion
- Midpoint integration

### ðŸŒªï¸ 2. Incompressible Euler
- Captures multi-scale vortex dynamics
- Outperforms PINNs and DeepONets under same memory limits

### ðŸ§± 3. Elastodynamic Simulations
- Models nonlinear deformations and contact
- Outperforms FEM and MPM in memory-constrained settings

---

## ðŸ“ˆ Benefits

| âœ… Advantages | âŒ Limitations |
|--------------|----------------|
| Mesh-free | Slower runtime |
| Memory efficient | Requires tuning |
| High spatial accuracy | Not real-time yet |
| Easy to integrate with classical schemes | More compute per step |

---

## ðŸ”š Conclusion

INSRs are a promising alternative to traditional numerical solvers, especially in memory-constrained or adaptive scenarios. They unify deep learning with classical numerical integration â€” unlocking new ways to simulate complex physical systems.

> ðŸ“Œ Paper: [Chen et al., ICML 2023](https://arxiv.org/abs/2210.00124)  
> ðŸŽ“ Presented at: Advanced Seminar, UniversitÃ¤t Stuttgart (2025)

---

## ðŸ“¬ Stay in Touch

Check out [About Me](/about/) or connect on [GitHub](https://github.com/aveen28) for more deep learning & scientific ML content.
